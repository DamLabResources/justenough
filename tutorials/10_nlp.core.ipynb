{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp.core\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nlp.core\n",
    "\n",
    "This module contains the core structures that are similar between NLP and GLP and helps to abstract similarities.\n",
    "Switching between DNA, protein, and English is just a matter of switching the internal models and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "\n",
    "from itertools import islice\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from umap import UMAP\n",
    "from fastai.text.all import *\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def mean_pooling_attention(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "\n",
    "def masked_concat_pool(output, mask, bptt):\n",
    "    \"Pool `MultiBatchEncoder` outputs into one vector [last_hidden, max_pool, avg_pool]\"\n",
    "    lens = output.shape[1] - mask.long().sum(dim=1)\n",
    "    last_lens = mask[:,-bptt:].long().sum(dim=1)\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).sum(dim=1)\n",
    "    avg_pool.div_(lens.type(avg_pool.dtype)[:,None])\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    \n",
    "    last_hidden = output[torch.arange(0, output.size(0)),-last_lens-1]\n",
    "    x = torch.cat([last_hidden, \n",
    "                   max_pool, avg_pool], 1) #Concat pooling.\n",
    "    x = torch.where(torch.isnan(x) | torch.isinf(x), torch.zeros_like(x), x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class PoolingLinearClassifier(Module):\n",
    "    \"Create a linear classifier with pooling\"\n",
    "    def __init__(self, dims, ps, bptt, y_range=None):\n",
    "        if len(ps) != len(dims)-1: raise ValueError(\"Number of layers and dropout values do not match.\")\n",
    "        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]\n",
    "        layers = [LinBnDrop(i, o, p=p, act=a) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.bptt = bptt\n",
    "\n",
    "    def forward(self, input):\n",
    "        out,mask = input\n",
    "        x = masked_concat_pool(out, mask, out.shape[1]-1)\n",
    "        x = self.layers(x)\n",
    "        return x, out, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class TopicModelingInterface(object):\n",
    "    \n",
    "    def __init__(self, tokenizer = None, model = None, model_name = None, bs=8,\n",
    "                 cluster_dim = 10, viz_dim = 2, device = 'cuda',\n",
    "                 min_cluster_size = 5, max_length = 512):\n",
    "        \n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "        \n",
    "        if model is None:\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "        self.bs = bs\n",
    "        self.device = device\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.viz_dim = viz_dim\n",
    "        self.cluster_dim = cluster_dim\n",
    "        \n",
    "        self.umap_cluster = UMAP(n_components=cluster_dim)\n",
    "        self.cluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "        self.umap_viz = UMAP(n_components=2)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def text2embed(self, text, bs=None):\n",
    "        if type(text) is str: return self.text2embed([text])[0]\n",
    "        \n",
    "        bs = self.bs if bs is None else bs\n",
    "        \n",
    "        \n",
    "        it = iter(text)\n",
    "        \n",
    "        out_data = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch = list(islice(it, bs))\n",
    "            while batch:\n",
    "                tokens = self.tokenizer(batch, return_tensors='pt', padding='max_length',\n",
    "                                        truncation = True,\n",
    "                                        max_length = self.max_length)\n",
    "                tokens.to(self.device)\n",
    "                res = self.model(**tokens)\n",
    "                #print(tokens['attention_mask'])\n",
    "                out_data.append(masked_concat_pool(res[0], tokens['attention_mask'].type(torch.bool), self.max_length))\n",
    "                \n",
    "                batch = list(islice(it, bs))\n",
    "                \n",
    "        return torch.vstack(out_data)\n",
    "    \n",
    "    \n",
    "    def embed2cluster(self, embed, fit = True):\n",
    "        \n",
    "        if fit:\n",
    "            clst_data = self.umap_cluster.fit_transform(embed)\n",
    "            self.cluster.fit(clst_data)\n",
    "            labels = self.cluster.labels_\n",
    "        else:\n",
    "            clst_data = self.umap_cluster.transform(embed)\n",
    "            labels, _ = hdbscan.approximate_predict(self.cluster, clst_data)\n",
    "            \n",
    "        return labels, clst_data\n",
    "    \n",
    "    \n",
    "    def embed2xy(self, embed, fit = True):\n",
    "        \n",
    "        if fit:\n",
    "            xy = self.umap_viz.fit_transform(embed)\n",
    "        else:\n",
    "            xy = self.umap_cluster.transform(embed)\n",
    "        return xy\n",
    "    \n",
    "        \n",
    "    \n",
    "    def process_df(self, df, col = 'text', fit = True):\n",
    "        \n",
    "        \n",
    "        emb = self.text2embed(df[col].fillna('').tolist())\n",
    "        \n",
    "        clusters, cluster_data = self.embed2cluster(emb.cpu().numpy(), fit = fit)\n",
    "        xy = self.embed2xy(emb.cpu().numpy(), fit = fit)\n",
    "        \n",
    "        ndf = pd.DataFrame({'cluster': clusters,\n",
    "                            'X': xy[:, 0],\n",
    "                            'Y': xy[:, 1],\n",
    "                            'label': [str(c) for c in clusters]}, index = df.index)\n",
    "        for n in range(self.cluster_dim):\n",
    "            ndf[f'd{n}'] = cluster_data[:, n]\n",
    "        \n",
    "        return ndf, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:justenough] *",
   "language": "python",
   "name": "conda-env-justenough-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
