{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp transforms.sequence\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforms.sequence\n",
    "\n",
    "Transforms useful for processing sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fastai.text.all import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace interface\n",
    "\n",
    "Many of the leading BERT embedding models are distributed as HuggingFace models.\n",
    "\n",
    "The `Pipeline` and `Transforms` below are used to help bridge the gap between fast.ai and HuggingFace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to deal with the tokenizer. As input it takes space-delimited AA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def space_adder(seq):\n",
    "    \n",
    "    return ' '.join(seq)\n",
    "\n",
    "\n",
    "class SpaceTransform(Transform):\n",
    "    \"\"\"Adds spaces between AAs for HuggingFace\"\"\"\n",
    "    \n",
    "    def encodes(self, x):\n",
    "        if type(x) == str:\n",
    "            return space_adder(x)\n",
    "        \n",
    "        return L(space_adder(seq) for seq in x)\n",
    "    \n",
    "    def decodes(self, x):\n",
    "        \n",
    "        if type(x) == str:\n",
    "            return \n",
    "        \n",
    "        return [seq.replace(' ', '') for seq in x]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(space_adder('MIVLR'), 'M I V L R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_tfm = SpaceTransform()\n",
    "\n",
    "pipe = Pipeline([space_tfm])\n",
    "\n",
    "tst = ['MIVLR', 'AAR']\n",
    "cor = ['M I V L R', 'A A R']\n",
    "\n",
    "test_eq(pipe(tst), cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a space-delimited AA string, it needs to pass through the HuggingFace `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'Rostlab/prot_bert'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a wrapper for an HF tokenizer that can process the sequences into integer tokens and attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class HFTokenizerWrapper(Transform):\n",
    "    \n",
    "    def __init__(self, tokenizer, tokens_only = True, \n",
    "                 truncation = True, max_length = 128,\n",
    "                 padding = 'max_length', \n",
    "                 skip_special_tokens = True,\n",
    "                 device = 'cuda'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens_only = tokens_only\n",
    "        self.truncation = truncation\n",
    "        self.max_length = max_length\n",
    "        self.padding = padding\n",
    "        self.skip_special_tokens = skip_special_tokens\n",
    "        self.device = device\n",
    "        \n",
    "    def encodes(self, x):\n",
    "        \n",
    "        if type(x) == str:\n",
    "            x = [x]\n",
    "            \n",
    "        tokenized = self.tokenizer(list(x), \n",
    "                                   return_tensors='pt', \n",
    "                                   padding=self.padding,\n",
    "                                   truncation = self.truncation,\n",
    "                                   max_length = self.max_length)\n",
    "        tokenized = tokenized.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.tokens_only:\n",
    "            return tokenized['input_ids']\n",
    "        else:\n",
    "            return [fastuple(tokenized['input_ids'][i], tokenized['attention_mask'][i]) for i in range(len(x))]\n",
    "        \n",
    "        \n",
    "    def decodes(self, x):\n",
    "        \n",
    "        return self.tokenizer.batch_decode(x, skip_special_tokens = self.skip_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tfm = HFTokenizerWrapper(tokenizer, max_length=7, device = 'cpu')\n",
    "\n",
    "test_eq(token_tfm('M I V L R'), tensor([[ 2, 21, 11,  8,  5, 13,  3]]))\n",
    "test_eq(token_tfm(['M I V L R']), tensor([[ 2, 21, 11,  8,  5, 13,  3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(token_tfm.decode([[ 2, 21, 11,  8,  5, 13,  3]]), ['M I V L R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tfm = HFTokenizerWrapper(tokenizer, max_length=7, device = 'cpu', tokens_only = False)\n",
    "\n",
    "\n",
    "test_eq(token_tfm('M I V L R'), \n",
    "        [fastuple(tensor([ 2, 21, 11,  8,  5, 13,  3]), tensor([ 1, 1, 1,  1,  1, 1,  1]))])\n",
    "\n",
    "test_eq(token_tfm(['M I V L']), \n",
    "        [fastuple(tensor([ 2, 21, 11,  8,  5, 3,  0]), tensor([ 1, 1, 1,  1,  1, 1,  0]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_tfm = SpaceTransform()\n",
    "token_tfm = HFTokenizerWrapper(tokenizer, max_length=7, device = 'cpu')\n",
    "pipe = Pipeline([space_tfm, token_tfm])\n",
    "\n",
    "tst = ['MIVLR', 'AAR']\n",
    "cor = [[2, 21, 11,  8, 5, 13, 3], \n",
    "       [2,  6,  6, 13, 3,  0, 0]]\n",
    "\n",
    "test_eq(pipe(tst), tensor(cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(pipe.decode(tensor(cor)), tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes instead of using the ProtBert model across all of your sequences everytime, you want to pre-process them using a `Transform`. This can DRASTICALLY speed up analysis if you never intend to train the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from justenough.models.glp import model_mask_pooling\n",
    "\n",
    "class HFPoolingTransform(Transform):\n",
    "    \n",
    "    def __init__(self, model, bs = 32):\n",
    "        \n",
    "        self.model = model\n",
    "        self.bs = bs\n",
    "    \n",
    "    def encodes(self, x):\n",
    "        \n",
    "        if type(x[0]) == fastuple:\n",
    "            input_ids, attention = zip(*x)\n",
    "            input_ids = torch.vstack(input_ids)\n",
    "            attention = torch.vstack(attention).type(torch.bool)\n",
    "        else:\n",
    "            input_ids = x\n",
    "            attention = x == 0\n",
    "            \n",
    "        out = model_mask_pooling(input_ids, attention, self.model, bs = self.bs)\n",
    "        #print(out.shape)\n",
    "        if out.shape[0] == 1:\n",
    "            out = torch.squeeze(out, 0)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = 'Rostlab/prot_bert'\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tfm = HFTokenizerWrapper(tokenizer, max_length=6, tokens_only=True, device = 'cpu')\n",
    "bert_pool_tfm = HFPoolingTransform(model)\n",
    "pipe = Pipeline([space_tfm, token_tfm, bert_pool_tfm])\n",
    "\n",
    "encoded = pipe(tst*100)\n",
    "test_eq(encoded.shape, (200, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tfm = HFTokenizerWrapper(tokenizer, max_length=6, tokens_only=False, device = 'cpu')\n",
    "bert_pool_tfm = HFPoolingTransform(model)\n",
    "pipe = Pipeline([space_tfm, token_tfm, bert_pool_tfm])\n",
    "\n",
    "encoded = pipe(tst*100)\n",
    "test_eq(encoded.shape, (200, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:justenough] *",
   "language": "python",
   "name": "conda-env-justenough-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
